{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that Hyperface with VGG16 as main model is so big, we decide to try to train in a 'smaller' architecture (SqueezeNet https://arxiv.org/abs/1602.07360v3).\n",
    "\n",
    "Reducing the size of AlexNet:\n",
    "- SVD: 5x compression, 56% top-1 accuracy\n",
    "- Pruning: 9x compression, 57.2% top-1 accuracy\n",
    "- Deep Compression: 35x compression, ~57% top-1 accuracy\n",
    "- SqueezeNet: 50x compression, ~57% top-1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import squeeze\n",
    "import hf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping classifier block in Squeeze model. The net already has pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = squeeze.SqueezeNet()\n",
    "model = Model(inputs=model.input, outputs=model.get_layer('drop9').output)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 27, 27, 128) -> (?, 6, 6, 128)\n",
      "(?, 13, 13, 256) -> (?, 6, 6, 128)\n",
      "(?, 13, 13, 512) -> (?, 6, 6, 128)\n",
      "(?, 6, 6, 384)\n",
      "(?, 4, 4, 128)\n",
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, Dropout, GlobalAveragePooling2D, merge\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "#\n",
    "# # fire2/concat (56x56x128) porque se asemeja a paper (27x27x96, aunque en imagen luce 51x51x96)\n",
    "# conv1a_input = model.get_layer('fire2/concat').output\n",
    "# # con kernel=9x9/9 llegamos a 6x6x128 en paper es 6x6x256\n",
    "# conv1a = Convolution2D(128, (9,9), strides=(9,9), activation='relu', padding='valid', name='conv1a')(conv1a_input)\n",
    "#\n",
    "# pool3 (27x27x128) se asemeja mejor \n",
    "conv1a_input = model.get_layer('pool3').output\n",
    "# con kernel=4x4/4 llegamos a 6x6x128 en paper es 6x6x256\n",
    "conv1a = Convolution2D(128, (4,4), strides=(4,4), activation='relu', padding='valid', name='conv1a')(conv1a_input)\n",
    "print(conv1a_input.shape, '->', conv1a.shape)\n",
    "\n",
    "# pool5 (13x13x256) paper (13x13x384)\n",
    "conv3a_input = model.get_layer('pool5').output\n",
    "conv3a = Convolution2D(128, (2,2), strides=(2,2), activation='relu', padding='valid', name='conv3a')(conv3a_input)\n",
    "print(conv3a_input.shape, '->', conv3a.shape)\n",
    "\n",
    "# no tiene nombre así que le nombramos conv5a\n",
    "# drop9 (13x13x512) paper (6x6x256, aunque en imagen luce 13x13x256)\n",
    "conv5a_input = model.get_layer('drop9').output\n",
    "conv5a = Convolution2D(128, (2,2), strides=(2,2), activation='relu', padding='valid', name='conv5a')(conv5a_input)\n",
    "print(conv5a_input.shape, '->', conv5a.shape)\n",
    "\n",
    "# combinación: 6x6x384 (paper: 6x6x768)\n",
    "concat = concatenate([conv1a, conv3a, conv5a], axis=-1, name='concat')\n",
    "print(concat.shape)\n",
    "\n",
    "# reducción de dimensión: 4x4x128 (paper: 6x6x192)\n",
    "# todo: obtendremos la misma accuracy que entrenando con kernel=(1,1) pero cambios en todas las conv precedentes para llegar a 4x4?\n",
    "# ya que es una fusión de características no parece buena idea mezclar todas haciendo una conv 3x3\n",
    "conv_all = Convolution2D(128, (3, 3), strides=(1,1), activation='relu', padding='valid', name='conv_all')(concat)\n",
    "print(conv_all.shape)\n",
    "\n",
    "# completamente conectadas\n",
    "fc_full = Flatten(input_shape=conv_all.shape, name='fc_full')(conv_all)\n",
    "print(fc_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<img src=\"squeezenet-architecture.png\"/>-->\n",
    "<img src=\"HyperFace-architecture.small.png\"/>\n",
    "It is important to use Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "# face/non-face\n",
    "fc_detecton = Dense(256, name='fc_detecton', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "face_nonface = Dense(2, name='face_nonface', activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_detecton)\n",
    "# print(fc_detecton.shape, face_nonface.shape)\n",
    "\n",
    "fc_landmarks = Dense(256, name='fc_landmarks', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "landmarks = Dense(42, name='landmarks', activation='linear', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_landmarks)\n",
    "# print(fc_landmarks.shape, landmarks.shape)\n",
    "\n",
    "fc_visibility = Dense(256, name='fc_visibility', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "visibility = Dense(21, name='visibility', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_visibility)\n",
    "# print(fc_visibility.shape, visibility.shape)\n",
    "\n",
    "fc_pose = Dense(256, name='fc_pose', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "roll_pitch_yaw = Dense(3, name='roll_pitch_yaw', activation='linear', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_pose)\n",
    "# print(fc_pose.shape, roll_pitch_yaw.shape)\n",
    "\n",
    "fc_gender = Dense(256, name='fc_gender', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "male_female = Dense(2, name='male_female', activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_gender)\n",
    "# print(fc_gender.shape, male_female.shape)\n",
    "\n",
    "hyperface = Model(inputs=model.input, outputs=[face_nonface, landmarks, visibility, roll_pitch_yaw, male_female])\n",
    "# hyperface.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperface model compiled\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as kb\n",
    "import keras.losses as losses\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def custom_loss_visibility(y_true, y_pred):\n",
    "    # en paper: loss = 1/N * sum((vpredi-vi)^2)\n",
    "    # para entrenar:\n",
    "    return (1/21) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/21) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "def custom_loss_pose(y_true, y_pred):\n",
    "    # en paper: loss = 1/3 * sum((ppredi-pi)^2)\n",
    "    # para entrenamiento:\n",
    "    return (1/3) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/3) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "def custom_loss_landmarks(coord_true, coord_pred):    \n",
    "    # en paper: loss = 1/(2N) * Sum(vi*((xpredi-ai)^2 + (ypredi-bi)^2))\n",
    "    x_true_coord = coord_true[:,0:21]\n",
    "    y_true_coord = coord_true[:,21:42]\n",
    "    # viz_true = coord_true[:,42:63]  # produce errores en otros entornos (versión de keras?)\n",
    "    x_pred_coord = coord_pred[:,0:21]\n",
    "    y_pred_coord = coord_pred[:,21:42]\n",
    "    # para entrenamiento:\n",
    "    # return (1/(2*21)) * kb.sum(viz_true * (kb.square(x_pred_coord-x_true_coord) + K.square(y_pred_coord - y_true_coord)), axis=-1)\n",
    "    return (1/(2*21)) * kb.sum((kb.square(x_pred_coord-x_true_coord) + K.square(y_pred_coord - y_true_coord)), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/(2*21)) * np.sum(viz_true * ((x_pred_coord-x_true_coord)**2 + (y_pred_coord - y_true_coord)**2), axis=-1)\n",
    "\n",
    "def custom_mse_lm(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))*kb.sum(kb.square(tf.multiply((kb.sign(y_true)+1)*0.5, y_true-y_pred)),axis=-1)/kb.sum((kb.sign(y_true)+1)*0.5,axis=-1)\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "hyperface.compile(optimizer=optimizer,\n",
    "                  loss={\n",
    "                      'face_nonface': 'categorical_crossentropy',\n",
    "                      'landmarks': custom_loss_landmarks,\n",
    "                      'visibility': custom_loss_visibility,\n",
    "                      'roll_pitch_yaw': custom_loss_pose,\n",
    "                      'male_female': 'categorical_crossentropy'},\n",
    "                  loss_weights={\n",
    "                      'face_nonface': 1,\n",
    "                      'landmarks': 1,\n",
    "                      'visibility': 1,\n",
    "                      'roll_pitch_yaw': 1,\n",
    "                      'male_female': 1})\n",
    "print(\"hyperface model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train/validation data generator...\n",
      "Found 2 positive samples and 2 negative samples.\n",
      "Start training...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 3s - loss: 0.3608 - face_nonface_loss: 0.0174 - landmarks_loss: 0.1371 - visibility_loss: 0.0553 - roll_pitch_yaw_loss: 0.1507 - male_female_loss: 3.2234e-04     \n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 3s - loss: 0.0585 - face_nonface_loss: 0.0029 - landmarks_loss: 0.0188 - visibility_loss: 0.0041 - roll_pitch_yaw_loss: 0.0327 - male_female_loss: 1.9663e-05     \n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 3s - loss: 0.0216 - face_nonface_loss: 0.0011 - landmarks_loss: 0.0052 - visibility_loss: 0.0014 - roll_pitch_yaw_loss: 0.0140 - male_female_loss: 3.5753e-06     \n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 3s - loss: 0.0090 - face_nonface_loss: 2.6323e-04 - landmarks_loss: 0.0017 - visibility_loss: 4.9901e-04 - roll_pitch_yaw_loss: 0.0065 - male_female_loss: 1.8367e-06     \n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 3s - loss: 0.0040 - face_nonface_loss: 1.1742e-04 - landmarks_loss: 4.5866e-04 - visibility_loss: 2.4599e-04 - roll_pitch_yaw_loss: 0.0032 - male_female_loss: 8.1092e-07     \n"
     ]
    }
   ],
   "source": [
    "import hf\n",
    "\n",
    "print(\"Building train/validation data generator...\")\n",
    "\n",
    "# para corregir el problema de una imagen jpg no válida:\n",
    "# convert -resize 50% /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg\n",
    "# convert -resize 200% /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg\n",
    "json_dir = os.path.dirname(os.path.realpath('__file__')) # genérico\n",
    "\n",
    "train_data = hf.ImageDataGeneratorV2(samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'negatives5k-train.json', 'positives5k-train.json', \n",
    "                                                 # 'positives.json', 'negatives.json', \n",
    "                                                 pos_max_load_labels=12116, neg_max_load_labels=12116*3, #Número de negativos 454512\n",
    "                                                 output_type='hyperface', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "val_data = hf.ImageDataGeneratorV2(samplewise_center=True,            \n",
    "                                    samplewise_std_normalization=True)\n",
    "val_data_flow = val_data.flow_from_directory(json_dir, \n",
    "                                              'negatives5k-test.json', 'positives5k-test.json', \n",
    "                                              pos_max_load_labels=505, neg_max_load_labels=505*3, #Número de negativos 18938\n",
    "                                              output_type='hyperface', target_size=(227, 227),\n",
    "                                              pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "# # checkpoint\n",
    "# filepath=\"weights-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='train_loss', verbose=1, save_best_only=False, mode='min', period=30)\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "print(\"Start training...\")\n",
    "# history = hyperface.fit_generator(train_data_flow, steps_per_epoch=100, epochs=300, callbacks=callbacks_list)\n",
    "history = hyperface.fit_generator(generator=train_data_flow, \n",
    "                                   validation_data=val_data_flow,\n",
    "                                   validation_steps=100,\n",
    "                                  steps_per_epoch=100, epochs=5) \n",
    "#                                   steps_per_epoch=100, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 positive samples and 2 negative samples.\n",
      "Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)\n",
      "face_nonface ground truth:\n",
      "[1 0]\n",
      "face_nonface predicted:\n",
      "[[  9.99896049e-01   1.03991566e-04]]\n",
      "\n",
      "landmarks ground truth:\n",
      "[-0.2704918  -0.17213115  0.00273224  0.14480874  0.21038251 -1.81693989\n",
      " -0.19398907 -0.1010929  -0.05191257 -1.81693989  0.1557377  -1.81693989\n",
      " -1.81693989 -0.06830601  0.12295082 -1.81693989 -1.81693989 -0.17213115\n",
      "  0.00819672 -1.81693989 -0.04644809 -0.3136646  -0.38819876 -0.32608696\n",
      " -0.30124224 -0.27639752 -0.59937888 -0.26397516 -0.26397516 -0.23291925\n",
      " -0.59937888 -0.18322981 -0.59937888 -0.59937888  0.02795031  0.02173913\n",
      " -0.59937888 -0.59937888  0.14596273  0.17080745 -0.59937888  0.44409938]\n",
      "landmarks predicted:\n",
      "[[-0.28080887 -0.18695293  0.005672    0.16222359  0.221444   -1.82775176\n",
      "  -0.18509698 -0.10352197 -0.05326626 -1.84291005  0.16400044 -1.82655728\n",
      "  -1.83404064 -0.05065612  0.12482188 -1.82384801 -1.82902491 -0.18534179\n",
      "  -0.0019758  -1.82659268 -0.03916674 -0.31576669 -0.39775622 -0.33913454\n",
      "  -0.30451676 -0.28307819 -0.6251902  -0.27568987 -0.2713398  -0.25139588\n",
      "  -0.58378732 -0.19671577 -0.57344294 -0.59810466  0.03717191  0.03525973\n",
      "  -0.61818218 -0.60269237  0.13436051  0.17963494 -0.60198748  0.43240952]]\n",
      "\n",
      "visibility ground truth:\n",
      "[ 1.  1.  1.  1.  1.  0.  1.  1.  1.  0.  1.  0.  0.  1.  1.  0.  0.  1.\n",
      "  1.  0.  1.]\n",
      "visibility predicted:\n",
      "[[ 0.9821701   0.98687452  0.98383904  0.98383224  0.9841727   0.01000969\n",
      "   0.98076874  0.98956072  0.98258948  0.00677984  0.98304445  0.0028725\n",
      "   0.00803579  0.98290676  0.9814949   0.00610287  0.01551899  0.9831714\n",
      "   0.98393202  0.00721117  0.98451644]]\n",
      "\n",
      "pose ground truth:\n",
      "[-0.12002859 -0.22056605 -0.40335208]\n",
      "pose predicted:\n",
      "[[-0.12361965 -0.19327033 -0.3982338 ]]\n",
      "\n",
      "gender ground truth:\n",
      "[0 1]\n",
      "gender predicted:\n",
      "[[  1.93119877e-06   9.99998093e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                            'negatives5k-train.json', 'positives5k-train.json', \n",
    "                                            # 'positives.json', 'negatives.json', \n",
    "                                            pos_max_load_labels=12116, neg_max_load_labels=12116*3, #Número de negativos 454512\n",
    "                                            output_type='predict', target_size=(227, 227),\n",
    "                                            pos_batch_size=64, neg_batch_size=64)\n",
    "batch_x, batch_image, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = data_flow.next()\n",
    "\n",
    "# DO NOT LOAD LIKE THIS, USE A REGION FIRST!\n",
    "# path = batch_image[0]\n",
    "# print(\"loading\", path)\n",
    "# img = image.load_img(path, target_size=(227, 227))\n",
    "# x = image.img_to_array(img)\n",
    "# x = train_data.standardize(x)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "\n",
    "subject = 0\n",
    "x = batch_x[subject]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = hyperface.predict(x)\n",
    "\n",
    "print('Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)')\n",
    "\n",
    "print('face_nonface ground truth:')\n",
    "print(batch_y_fnf[subject])\n",
    "print('face_nonface predicted:')\n",
    "print(preds[0])\n",
    "print()\n",
    "\n",
    "print('landmarks ground truth:')\n",
    "print(batch_y_landmarks[subject])\n",
    "print('landmarks predicted:')\n",
    "print(preds[1])\n",
    "print()\n",
    "\n",
    "print('visibility ground truth:')\n",
    "print(batch_y_visfac[subject])\n",
    "print('visibility predicted:')\n",
    "print(preds[2])\n",
    "print()\n",
    "\n",
    "print('pose ground truth:')\n",
    "print(batch_y_pose[subject])\n",
    "print('pose predicted:')\n",
    "print(preds[3])\n",
    "print()\n",
    "\n",
    "print('gender ground truth:')\n",
    "print(batch_y_gender[subject])\n",
    "print('gender predicted:')\n",
    "print(preds[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0298756b38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAEICAYAAABcYjLsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02987560f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(history.history)\n",
    "# df.plot(y=['loss', 'val_loss'], figsize=(16,4), title='Loss')\n",
    "# df.plot(y=['acc', 'val_acc'], figsize=(16,4), title='Accuracy')\n",
    "df.plot(y=['loss'], figsize=(12,4), title='Loss')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "hyperface.save_weights(\"hyperface.weights.h5\")\n",
    "\n",
    "model_yaml = hyperface.to_yaml()\n",
    "with open(\"hyperface.model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "# throws json non serializable exception\n",
    "# model_json = hyperface.to_json()\n",
    "# with open(\"hyperface.model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "print(\"model written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a model without heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperface_faceposegender model compiled\n",
      "Found 2 positive samples and 2 negative samples.\n",
      "training...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1951e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n"
     ]
    }
   ],
   "source": [
    "hyperface_faceposegender = Model(inputs=model.input, outputs=[face_nonface, roll_pitch_yaw, male_female])\n",
    "optimizer = Adam(lr=0.0001)\n",
    "hyperface_faceposegender.compile(optimizer=optimizer,\n",
    "                  loss={\n",
    "                      'face_nonface': 'categorical_crossentropy',\n",
    "                      'roll_pitch_yaw': custom_loss_pose,\n",
    "                      'male_female': 'categorical_crossentropy'},\n",
    "                  loss_weights={\n",
    "                      'face_nonface': 1,\n",
    "                      'roll_pitch_yaw': 1,\n",
    "                      'male_female': 1})\n",
    "print(\"hyperface_faceposegender model compiled\")\n",
    "\n",
    "json_dir = os.path.dirname(os.path.realpath('__file__')) # genérico\n",
    "train_data = hf.ImageDataGeneratorV2(samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json3k', 'negatives.json3k', \n",
    "                                                 # 'positives.json', 'negatives.json', \n",
    "                                                 pos_max_load_labels=2, neg_max_load_labels=2,\n",
    "                                                 output_type='faceposegender', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "print(\"training...\")\n",
    "history = hyperface_faceposegender.fit_generator(generator=train_data_flow, \n",
    "                                  steps_per_epoch=100, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 positive samples and 1 negative samples.\n",
      "Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)\n",
      "face_nonface ground truth:\n",
      "[1 0]\n",
      "face_nonface predicted:\n",
      "[[  9.86067116e-01   6.11221317e-12]]\n",
      "\n",
      "pose ground truth:\n",
      "[-0.12002859 -0.22056605 -0.40335208]\n",
      "pose predicted:\n",
      "[[  2.77214002e-10   3.17766813e-10   7.23532623e-11]]\n",
      "\n",
      "gender ground truth:\n",
      "[0 1]\n",
      "gender predicted:\n",
      "[[  6.42394287e-13   1.00000000e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                           'positives.json3k', 'negatives.json3k', \n",
    "                                           # 'positives.json', 'negatives.json', \n",
    "                                           pos_max_load_labels=1, neg_max_load_labels=1,\n",
    "                                           output_type='predict', target_size=(227, 227),\n",
    "                                           pos_batch_size=64, neg_batch_size=64)\n",
    "batch_x, batch_image, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = data_flow.next()\n",
    "\n",
    "# DO NOT LOAD LIKE THIS, USE A REGION FIRST!\n",
    "# path = batch_image[0]\n",
    "# print(\"loading\", path)\n",
    "# img = image.load_img(path, target_size=(227, 227))\n",
    "# x = image.img_to_array(img)\n",
    "# x = train_data.standardize(x)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "\n",
    "x = batch_x[0]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = hyperface_faceposegender.predict(x)\n",
    "\n",
    "print('Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)')\n",
    "\n",
    "print('face_nonface ground truth:')\n",
    "print(batch_y_fnf[0])\n",
    "print('face_nonface predicted:')\n",
    "print(preds[0])\n",
    "print()\n",
    "\n",
    "print('pose ground truth:')\n",
    "print(batch_y_pose[0])\n",
    "print('pose predicted:')\n",
    "print(preds[1])\n",
    "print()\n",
    "\n",
    "print('gender ground truth:')\n",
    "print(batch_y_gender[0])\n",
    "print('gender predicted:')\n",
    "print(preds[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02123045', 'tabby', 0.8085466), ('n02123159', 'tiger_cat', 0.14428307), ('n02124075', 'Egyptian_cat', 0.043808475), ('n02971356', 'carton', 0.0009396731), ('n02127052', 'lynx', 0.00060345326)]]\n"
     ]
    }
   ],
   "source": [
    "model = squeeze.SqueezeNet()\n",
    "\n",
    "path = '/home/lmiguel/Projects/datasets/coco/images/test2017/000000000665.jpg'\n",
    "img = image.load_img(path, target_size=(227, 227))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
