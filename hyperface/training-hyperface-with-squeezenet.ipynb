{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that Hyperface with VGG16 as main model is so big, we decide to try to train in a 'smaller' architecture (SqueezeNet https://arxiv.org/abs/1602.07360v3).\n",
    "\n",
    "Reducing the size of AlexNet:\n",
    "- SVD: 5x compression, 56% top-1 accuracy\n",
    "- Pruning: 9x compression, 57.2% top-1 accuracy\n",
    "- Deep Compression: 35x compression, ~57% top-1 accuracy\n",
    "- SqueezeNet: 50x compression, ~57% top-1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import squeeze\n",
    "import hf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping classifier block in Squeeze model. The net already has pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = squeeze.SqueezeNet()\n",
    "model = Model(inputs=model.input, outputs=model.get_layer('drop9').output)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 27, 27, 128) -> (?, 6, 6, 128)\n",
      "(?, 13, 13, 256) -> (?, 6, 6, 128)\n",
      "(?, 13, 13, 512) -> (?, 6, 6, 128)\n",
      "(?, 6, 6, 384)\n",
      "(?, 4, 4, 128)\n",
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, Dropout, GlobalAveragePooling2D, merge\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "#\n",
    "# # fire2/concat (56x56x128) porque se asemeja a paper (27x27x96, aunque en imagen luce 51x51x96)\n",
    "# conv1a_input = model.get_layer('fire2/concat').output\n",
    "# # con kernel=9x9/9 llegamos a 6x6x128 en paper es 6x6x256\n",
    "# conv1a = Convolution2D(128, (9,9), strides=(9,9), activation='relu', padding='valid', name='conv1a')(conv1a_input)\n",
    "#\n",
    "# pool3 (27x27x128) se asemeja mejor \n",
    "conv1a_input = model.get_layer('pool3').output\n",
    "# con kernel=4x4/4 llegamos a 6x6x128 en paper es 6x6x256\n",
    "conv1a = Convolution2D(128, (4,4), strides=(4,4), activation='relu', padding='valid', name='conv1a')(conv1a_input)\n",
    "print(conv1a_input.shape, '->', conv1a.shape)\n",
    "\n",
    "# pool5 (13x13x256) paper (13x13x384)\n",
    "conv3a_input = model.get_layer('pool5').output\n",
    "conv3a = Convolution2D(128, (2,2), strides=(2,2), activation='relu', padding='valid', name='conv3a')(conv3a_input)\n",
    "print(conv3a_input.shape, '->', conv3a.shape)\n",
    "\n",
    "# no tiene nombre así que le nombramos conv5a\n",
    "# drop9 (13x13x512) paper (6x6x256, aunque en imagen luce 13x13x256)\n",
    "conv5a_input = model.get_layer('drop9').output\n",
    "conv5a = Convolution2D(128, (2,2), strides=(2,2), activation='relu', padding='valid', name='conv5a')(conv5a_input)\n",
    "print(conv5a_input.shape, '->', conv5a.shape)\n",
    "\n",
    "# combinación: 6x6x384 (paper: 6x6x768)\n",
    "concat = concatenate([conv1a, conv3a, conv5a], axis=-1, name='concat')\n",
    "print(concat.shape)\n",
    "\n",
    "# reducción de dimensión: 4x4x128 (paper: 6x6x192)\n",
    "# todo: obtendremos la misma accuracy que entrenando con kernel=(1,1) pero cambios en todas las conv precedentes para llegar a 4x4?\n",
    "# ya que es una fusión de características no parece buena idea mezclar todas haciendo una conv 3x3\n",
    "conv_all = Convolution2D(128, (3, 3), strides=(1,1), activation='relu', padding='valid', name='conv_all')(concat)\n",
    "print(conv_all.shape)\n",
    "\n",
    "# completamente conectadas\n",
    "fc_full = Flatten(input_shape=conv_all.shape, name='fc_full')(conv_all)\n",
    "print(fc_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<img src=\"squeezenet-architecture.png\"/>-->\n",
    "<img src=\"HyperFace-architecture.small.png\"/>\n",
    "It is important to use Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "# face/non-face\n",
    "fc_detecton = Dense(256, name='fc_detecton', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "face_nonface = Dense(2, name='face_nonface', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_detecton)\n",
    "# print(fc_detecton.shape, face_nonface.shape)\n",
    "\n",
    "fc_landmarks = Dense(256, name='fc_landmarks', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "landmarks = Dense(42, name='landmarks', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_landmarks)\n",
    "# print(fc_landmarks.shape, landmarks.shape)\n",
    "\n",
    "fc_visibility = Dense(256, name='fc_visibility', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "visibility = Dense(21, name='visibility', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_visibility)\n",
    "# print(fc_visibility.shape, visibility.shape)\n",
    "\n",
    "fc_pose = Dense(256, name='fc_pose', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "roll_pitch_yaw = Dense(3, name='roll_pitch_yaw', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_pose)\n",
    "# print(fc_pose.shape, roll_pitch_yaw.shape)\n",
    "\n",
    "fc_gender = Dense(256, name='fc_gender', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "male_female = Dense(2, name='male_female', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_gender)\n",
    "# print(fc_gender.shape, male_female.shape)\n",
    "\n",
    "hyperface = Model(inputs=model.input, outputs=[face_nonface, landmarks, visibility, roll_pitch_yaw, male_female])\n",
    "# hyperface.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperface model compiled\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as kb\n",
    "import keras.losses as losses\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def custom_loss_visibility(y_true, y_pred):\n",
    "    # en paper: loss = 1/N * sum((vpredi-vi)^2)\n",
    "    # para entrenar:\n",
    "    return (1/21) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/21) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "def custom_loss_pose(y_true, y_pred):\n",
    "    # en paper: loss = 1/3 * sum((ppredi-pi)^2)\n",
    "    # para entrenamiento:\n",
    "    return (1/3) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/3) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "def custom_loss_landmarks(coord_true, coord_pred):    \n",
    "    # en paper: loss = 1/(2N) * Sum(vi*((xpredi-ai)^2 + (ypredi-bi)^2))\n",
    "    x_true_coord = coord_true[:,0:21]\n",
    "    y_true_coord = coord_true[:,21:42]\n",
    "    viz_true = coord_true[:,42:63]\n",
    "    x_pred_coord = coord_pred[:,0:21]\n",
    "    y_pred_coord = coord_pred[:,21:42]\n",
    "    # para entrenamiento:\n",
    "    return (1/(2*21)) * kb.sum(viz_true * (kb.square(x_pred_coord-x_true_coord) + K.square(y_pred_coord - y_true_coord)), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/(2*21)) * np.sum(viz_true * ((x_pred_coord-x_true_coord)**2 + (y_pred_coord - y_true_coord)**2), axis=-1)\n",
    "\n",
    "def custom_mse_lm(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))*kb.sum(kb.square(tf.multiply((kb.sign(y_true)+1)*0.5, y_true-y_pred)),axis=-1)/kb.sum((kb.sign(y_true)+1)*0.5,axis=-1)\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "hyperface.compile(optimizer=optimizer,\n",
    "                  loss={\n",
    "                      'face_nonface': 'categorical_crossentropy',\n",
    "                      'landmarks': custom_loss_landmarks,\n",
    "                      'visibility': custom_loss_visibility,\n",
    "                      'roll_pitch_yaw': custom_loss_pose,\n",
    "                      'male_female': 'categorical_crossentropy'},\n",
    "                  loss_weights={\n",
    "                      'face_nonface': 1,\n",
    "                      'landmarks': 1,\n",
    "                      'visibility': 1,\n",
    "                      'roll_pitch_yaw': 1,\n",
    "                      'male_female': 1})\n",
    "print(\"hyperface model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train/validation data generator...\n",
      "Found 2 positive samples and 2 negative samples.\n",
      "Start training...\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 8.6176e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 7.7151e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 6.5924e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 5.6888e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 5.2310e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 4.5785e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 4.5847e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 4.1503e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 3.6445e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 3.4034e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 3.2640e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 2.8654e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 2.8976e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 2.7714e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 2.5466e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 2.1595e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.7664e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 2.0574e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.8482e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.5163e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.7090e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.6120e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.5559e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.4379e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.3197e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.1929e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.2059e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.0396e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 1.0261e-08 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 3s - loss: 0.0272 - face_nonface_loss: 1.1921e-07 - landmarks_loss: 0.0084 - visibility_loss: 9.2848e-09 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n"
     ]
    }
   ],
   "source": [
    "import hf\n",
    "\n",
    "print(\"Building train/validation data generator...\")\n",
    "\n",
    "# para corregir el problema de una imagen jpg no válida:\n",
    "# convert -resize 50% /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg\n",
    "# convert -resize 200% /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg\n",
    "json_dir = os.path.dirname(os.path.realpath('__file__')) # genérico\n",
    "\n",
    "train_data = hf.ImageDataGeneratorV2(samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json3k', 'negatives.json3k', \n",
    "                                                 # 'positives.json', 'negatives.json', \n",
    "                                                 pos_max_load_labels=2, neg_max_load_labels=2,\n",
    "                                                 output_type='hyperface', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "# val_data = hf.ImageDataGeneratorV2(samplewise_center=True,            \n",
    "#                                    samplewise_std_normalization=True)\n",
    "# val_data_flow = val_data.flow_from_directory(json_dir, \n",
    "#                                              'positives.json', 'negatives.json', \n",
    "#                                              output_type='hyperface', target_size=(227, 227),\n",
    "#                                              pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "# # checkpoint\n",
    "# filepath=\"weights-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='train_loss', verbose=1, save_best_only=False, mode='min', period=30)\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "print(\"Start training...\")\n",
    "# history = hyperface.fit_generator(train_data_flow, steps_per_epoch=100, epochs=300, callbacks=callbacks_list)\n",
    "history = hyperface.fit_generator(generator=train_data_flow, \n",
    "#                                   validation_data=val_data_flow,\n",
    "#                                   validation_steps=100,\n",
    "                                  # steps_per_epoch=10, epochs=1) \n",
    "                                  steps_per_epoch=100, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                           'positives.json3k', 'negatives.json3k', \n",
    "                                           # 'positives.json', 'negatives.json', \n",
    "                                           pos_max_load_labels=1, neg_max_load_labels=1,\n",
    "                                           output_type='predict', target_size=(227, 227),\n",
    "                                           pos_batch_size=64, neg_batch_size=64)\n",
    "batch_x, batch_image, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = data_flow.next()\n",
    "\n",
    "# DO NOT LOAD LIKE THIS, USE A REGION FIRST!\n",
    "# path = batch_image[0]\n",
    "# print(\"loading\", path)\n",
    "# img = image.load_img(path, target_size=(227, 227))\n",
    "# x = image.img_to_array(img)\n",
    "# x = train_data.standardize(x)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "\n",
    "x = batch_x[0]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = hyperface.predict(x)\n",
    "\n",
    "print('Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)')\n",
    "\n",
    "print('face_nonface ground truth:')\n",
    "print(batch_y_fnf[0])\n",
    "print('face_nonface predicted:')\n",
    "print(preds[0])\n",
    "print()\n",
    "\n",
    "print('landmarks ground truth:')\n",
    "print(batch_y_landmarks[0])\n",
    "print('landmarks predicted:')\n",
    "print(preds[1])\n",
    "print()\n",
    "\n",
    "print('visibility ground truth:')\n",
    "print(batch_y_visfac[0])\n",
    "print('visibility predicted:')\n",
    "print(preds[2])\n",
    "print()\n",
    "\n",
    "print('pose ground truth:')\n",
    "print(batch_y_pose[0])\n",
    "print('pose predicted:')\n",
    "print(preds[3])\n",
    "print()\n",
    "\n",
    "print('gender ground truth:')\n",
    "print(batch_y_gender[0])\n",
    "print('gender predicted:')\n",
    "print(preds[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f65a65d9ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAEICAYAAABLQKIlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0AsKMwAAANCL/w8Rfu71L1Hl8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65a65d9080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(history.history)\n",
    "# df.plot(y=['loss', 'val_loss'], figsize=(16,4), title='Loss')\n",
    "# df.plot(y=['acc', 'val_acc'], figsize=(16,4), title='Accuracy')\n",
    "df.plot(y=['loss'], figsize=(12,4), title='Loss')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hyperface.save_weights(\"hyperface.weights.h5\")\n",
    "\n",
    "model_yaml = hyperface.to_yaml()\n",
    "with open(\"hyperface.model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "# throws json non serializable exception\n",
    "# model_json = hyperface.to_json()\n",
    "# with open(\"hyperface.model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "print(\"model written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a model without heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperface_faceposegender model compiled\n",
      "Found 2 positive samples and 2 negative samples.\n",
      "training...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1951e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n"
     ]
    }
   ],
   "source": [
    "hyperface_faceposegender = Model(inputs=model.input, outputs=[face_nonface, roll_pitch_yaw, male_female])\n",
    "optimizer = Adam(lr=0.0001)\n",
    "hyperface_faceposegender.compile(optimizer=optimizer,\n",
    "                  loss={\n",
    "                      'face_nonface': 'categorical_crossentropy',\n",
    "                      'roll_pitch_yaw': custom_loss_pose,\n",
    "                      'male_female': 'categorical_crossentropy'},\n",
    "                  loss_weights={\n",
    "                      'face_nonface': 1,\n",
    "                      'roll_pitch_yaw': 1,\n",
    "                      'male_female': 1})\n",
    "print(\"hyperface_faceposegender model compiled\")\n",
    "\n",
    "json_dir = os.path.dirname(os.path.realpath('__file__')) # genérico\n",
    "train_data = hf.ImageDataGeneratorV2(samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json3k', 'negatives.json3k', \n",
    "                                                 # 'positives.json', 'negatives.json', \n",
    "                                                 pos_max_load_labels=2, neg_max_load_labels=2,\n",
    "                                                 output_type='faceposegender', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "print(\"training...\")\n",
    "history = hyperface_faceposegender.fit_generator(generator=train_data_flow, \n",
    "                                  steps_per_epoch=100, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 positive samples and 1 negative samples.\n",
      "Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)\n",
      "face_nonface ground truth:\n",
      "[1 0]\n",
      "face_nonface predicted:\n",
      "[[  9.86067116e-01   6.11221317e-12]]\n",
      "\n",
      "pose ground truth:\n",
      "[-0.12002859 -0.22056605 -0.40335208]\n",
      "pose predicted:\n",
      "[[  2.77214002e-10   3.17766813e-10   7.23532623e-11]]\n",
      "\n",
      "gender ground truth:\n",
      "[0 1]\n",
      "gender predicted:\n",
      "[[  6.42394287e-13   1.00000000e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                           'positives.json3k', 'negatives.json3k', \n",
    "                                           # 'positives.json', 'negatives.json', \n",
    "                                           pos_max_load_labels=1, neg_max_load_labels=1,\n",
    "                                           output_type='predict', target_size=(227, 227),\n",
    "                                           pos_batch_size=64, neg_batch_size=64)\n",
    "batch_x, batch_image, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = data_flow.next()\n",
    "\n",
    "# DO NOT LOAD LIKE THIS, USE A REGION FIRST!\n",
    "# path = batch_image[0]\n",
    "# print(\"loading\", path)\n",
    "# img = image.load_img(path, target_size=(227, 227))\n",
    "# x = image.img_to_array(img)\n",
    "# x = train_data.standardize(x)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "\n",
    "x = batch_x[0]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = hyperface_faceposegender.predict(x)\n",
    "\n",
    "print('Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)')\n",
    "\n",
    "print('face_nonface ground truth:')\n",
    "print(batch_y_fnf[0])\n",
    "print('face_nonface predicted:')\n",
    "print(preds[0])\n",
    "print()\n",
    "\n",
    "print('pose ground truth:')\n",
    "print(batch_y_pose[0])\n",
    "print('pose predicted:')\n",
    "print(preds[1])\n",
    "print()\n",
    "\n",
    "print('gender ground truth:')\n",
    "print(batch_y_gender[0])\n",
    "print('gender predicted:')\n",
    "print(preds[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02123045', 'tabby', 0.8085466), ('n02123159', 'tiger_cat', 0.14428307), ('n02124075', 'Egyptian_cat', 0.043808475), ('n02971356', 'carton', 0.0009396731), ('n02127052', 'lynx', 0.00060345326)]]\n"
     ]
    }
   ],
   "source": [
    "model = squeeze.SqueezeNet()\n",
    "\n",
    "path = '/home/lmiguel/Projects/datasets/coco/images/test2017/000000000665.jpg'\n",
    "img = image.load_img(path, target_size=(227, 227))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
