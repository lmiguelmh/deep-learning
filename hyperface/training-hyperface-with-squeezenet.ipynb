{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that Hyperface with VGG16 as main model is so big, we decide to try to train in a 'smaller' architecture (SqueezeNet https://arxiv.org/abs/1602.07360v3).\n",
    "\n",
    "Reducing the size of AlexNet:\n",
    "- SVD: 5x compression, 56% top-1 accuracy\n",
    "- Pruning: 9x compression, 57.2% top-1 accuracy\n",
    "- Deep Compression: 35x compression, ~57% top-1 accuracy\n",
    "- SqueezeNet: 50x compression, ~57% top-1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import squeeze\n",
    "import hf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping classifier block in Squeeze model. The net already has pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = squeeze.SqueezeNet()\n",
    "model = Model(inputs=model.input, outputs=model.get_layer('drop9').output)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 27, 27, 128) -> (?, 6, 6, 128)\n",
      "(?, 13, 13, 256) -> (?, 6, 6, 128)\n",
      "(?, 13, 13, 512) -> (?, 6, 6, 128)\n",
      "(?, 6, 6, 384)\n",
      "(?, 4, 4, 128)\n",
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, Dropout, GlobalAveragePooling2D, merge\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "#\n",
    "# # fire2/concat (56x56x128) porque se asemeja a paper (27x27x96, aunque en imagen luce 51x51x96)\n",
    "# conv1a_input = model.get_layer('fire2/concat').output\n",
    "# # con kernel=9x9/9 llegamos a 6x6x128 en paper es 6x6x256\n",
    "# conv1a = Convolution2D(128, (9,9), strides=(9,9), activation='relu', padding='valid', name='conv1a')(conv1a_input)\n",
    "#\n",
    "# pool3 (27x27x128) se asemeja mejor \n",
    "conv1a_input = model.get_layer('pool3').output\n",
    "# con kernel=4x4/4 llegamos a 6x6x128 en paper es 6x6x256\n",
    "conv1a = Convolution2D(128, (4,4), strides=(4,4), activation='relu', padding='valid', name='conv1a')(conv1a_input)\n",
    "print(conv1a_input.shape, '->', conv1a.shape)\n",
    "\n",
    "# pool5 (13x13x256) paper (13x13x384)\n",
    "conv3a_input = model.get_layer('pool5').output\n",
    "conv3a = Convolution2D(128, (2,2), strides=(2,2), activation='relu', padding='valid', name='conv3a')(conv3a_input)\n",
    "print(conv3a_input.shape, '->', conv3a.shape)\n",
    "\n",
    "# no tiene nombre así que le nombramos conv5a\n",
    "# drop9 (13x13x512) paper (6x6x256, aunque en imagen luce 13x13x256)\n",
    "conv5a_input = model.get_layer('drop9').output\n",
    "conv5a = Convolution2D(128, (2,2), strides=(2,2), activation='relu', padding='valid', name='conv5a')(conv5a_input)\n",
    "print(conv5a_input.shape, '->', conv5a.shape)\n",
    "\n",
    "# combinación: 6x6x384 (paper: 6x6x768)\n",
    "concat = concatenate([conv1a, conv3a, conv5a], axis=-1, name='concat')\n",
    "print(concat.shape)\n",
    "\n",
    "# reducción de dimensión: 4x4x128 (paper: 6x6x192)\n",
    "# todo: obtendremos la misma accuracy que entrenando con kernel=(1,1) pero cambios en todas las conv precedentes para llegar a 4x4?\n",
    "# ya que es una fusión de características no parece buena idea mezclar todas haciendo una conv 3x3\n",
    "conv_all = Convolution2D(128, (3, 3), strides=(1,1), activation='relu', padding='valid', name='conv_all')(concat)\n",
    "print(conv_all.shape)\n",
    "\n",
    "# completamente conectadas\n",
    "fc_full = Flatten(input_shape=conv_all.shape, name='fc_full')(conv_all)\n",
    "print(fc_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<img src=\"squeezenet-architecture.png\"/>-->\n",
    "<img src=\"HyperFace-architecture.small.png\"/>\n",
    "It is important to use Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "# face/non-face\n",
    "fc_detecton = Dense(256, name='fc_detecton', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "face_nonface = Dense(2, name='face_nonface', activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_detecton)\n",
    "# print(fc_detecton.shape, face_nonface.shape)\n",
    "\n",
    "fc_landmarks = Dense(256, name='fc_landmarks', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "landmarks = Dense(42, name='landmarks', activation='linear', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_landmarks)\n",
    "# print(fc_landmarks.shape, landmarks.shape)\n",
    "\n",
    "fc_visibility = Dense(256, name='fc_visibility', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "visibility = Dense(21, name='visibility', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_visibility)\n",
    "# print(fc_visibility.shape, visibility.shape)\n",
    "\n",
    "fc_pose = Dense(256, name='fc_pose', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "roll_pitch_yaw = Dense(3, name='roll_pitch_yaw', activation='linear', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_pose)\n",
    "# print(fc_pose.shape, roll_pitch_yaw.shape)\n",
    "\n",
    "fc_gender = Dense(256, name='fc_gender', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_full)\n",
    "male_female = Dense(2, name='male_female', activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.00))(fc_gender)\n",
    "# print(fc_gender.shape, male_female.shape)\n",
    "\n",
    "hyperface = Model(inputs=model.input, outputs=[face_nonface, landmarks, visibility, roll_pitch_yaw, male_female])\n",
    "# hyperface.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperface model compiled\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as kb\n",
    "import keras.losses as losses\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def custom_loss_visibility(y_true, y_pred):\n",
    "    # en paper: loss = 1/N * sum((vpredi-vi)^2)\n",
    "    # para entrenar:\n",
    "    return (1/21) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/21) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "def custom_loss_pose(y_true, y_pred):\n",
    "    # en paper: loss = 1/3 * sum((ppredi-pi)^2)\n",
    "    # para entrenamiento:\n",
    "    return (1/3) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/3) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "def custom_loss_landmarks(coord_true, coord_pred):    \n",
    "    # en paper: loss = 1/(2N) * Sum(vi*((xpredi-ai)^2 + (ypredi-bi)^2))\n",
    "    x_true_coord = coord_true[:,0:21]\n",
    "    y_true_coord = coord_true[:,21:42]\n",
    "    # viz_true = coord_true[:,42:63]  # produce errores en otros entornos (versión de keras?)\n",
    "    x_pred_coord = coord_pred[:,0:21]\n",
    "    y_pred_coord = coord_pred[:,21:42]\n",
    "    # para entrenamiento:\n",
    "    # return (1/(2*21)) * kb.sum(viz_true * (kb.square(x_pred_coord-x_true_coord) + K.square(y_pred_coord - y_true_coord)), axis=-1)\n",
    "    return (1/(2*21)) * kb.sum((kb.square(x_pred_coord-x_true_coord) + K.square(y_pred_coord - y_true_coord)), axis=-1)\n",
    "    # para debug:\n",
    "    # return (1/(2*21)) * np.sum(viz_true * ((x_pred_coord-x_true_coord)**2 + (y_pred_coord - y_true_coord)**2), axis=-1)\n",
    "\n",
    "def custom_mse_lm(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))*kb.sum(kb.square(tf.multiply((kb.sign(y_true)+1)*0.5, y_true-y_pred)),axis=-1)/kb.sum((kb.sign(y_true)+1)*0.5,axis=-1)\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "hyperface.compile(optimizer=optimizer,\n",
    "                  loss={\n",
    "                      'face_nonface': 'categorical_crossentropy',\n",
    "                      'landmarks': custom_loss_landmarks,\n",
    "                      'visibility': custom_loss_visibility,\n",
    "                      'roll_pitch_yaw': custom_loss_pose,\n",
    "                      'male_female': 'categorical_crossentropy'},\n",
    "                  loss_weights={\n",
    "                      'face_nonface': 1,\n",
    "                      'landmarks': 1,\n",
    "                      'visibility': 1,\n",
    "                      'roll_pitch_yaw': 1,\n",
    "                      'male_female': 1})\n",
    "print(\"hyperface model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train/validation data generator...\n",
      "Found 12116 positive samples and 12116 negative samples.\n",
      "Found 505 positive samples and 505 negative samples.\n",
      "Start training...\n",
      "Epoch 1/25\n",
      "  5/100 [>.............................] - ETA: 15:40 - loss: 4.4623 - face_nonface_loss: 0.0148 - landmarks_loss: 2.5882 - visibility_loss: 0.2440 - roll_pitch_yaw_loss: 1.6152 - male_female_loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda2\\envs\\keras\\lib\\site-packages\\PIL\\TiffImagePlugin.py:725: UserWarning: Possibly corrupt EXIF data.  Expecting to read 19660800 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "C:\\Users\\Asus\\Anaconda2\\envs\\keras\\lib\\site-packages\\PIL\\TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 6. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12/100 [==>...........................] - ETA: 13:16 - loss: 2.3934 - face_nonface_loss: 0.0090 - landmarks_loss: 1.2979 - visibility_loss: 0.1552 - roll_pitch_yaw_loss: 0.9314 - male_female_loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda2\\envs\\keras\\lib\\site-packages\\PIL\\TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36/100 [=========>....................] - ETA: 9:17 - loss: 0.9425 - face_nonface_loss: 0.0081 - landmarks_loss: 0.4788 - visibility_loss: 0.0923 - roll_pitch_yaw_loss: 0.3632 - male_female_loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda2\\envs\\keras\\lib\\site-packages\\PIL\\TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77/100 [======================>.......] - ETA: 3:19 - loss: 0.4702 - face_nonface_loss: 0.0038 - landmarks_loss: 0.2350 - visibility_loss: 0.0469 - roll_pitch_yaw_loss: 0.1845 - male_female_loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda2\\envs\\keras\\lib\\site-packages\\PIL\\TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 4. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1259s 13s/step - loss: 0.3656 - face_nonface_loss: 0.0030 - landmarks_loss: 0.1818 - visibility_loss: 0.0364 - roll_pitch_yaw_loss: 0.1444 - male_female_loss: 0.0000e+00 - val_loss: 0.0053 - val_face_nonface_loss: 2.8573e-05 - val_landmarks_loss: 3.3758e-04 - val_visibility_loss: 0.0010 - val_roll_pitch_yaw_loss: 0.0039 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 2/25\n",
      " 10/100 [==>...........................] - ETA: 12:38 - loss: 0.0086 - face_nonface_loss: 1.4834e-05 - landmarks_loss: 8.1649e-04 - visibility_loss: 6.8650e-04 - roll_pitch_yaw_loss: 0.0070 - male_female_loss: 0.0000e+00Warning: Couldn't load image C://Users//Asus//AnacondaProjects//AFLW//aflw//data//flickr\\2/image09437.jpg\n",
      " 70/100 [====================>.........] - ETA: 4:16 - loss: 0.0052 - face_nonface_loss: 9.4725e-06 - landmarks_loss: 2.5461e-04 - visibility_loss: 3.9656e-04 - roll_pitch_yaw_loss: 0.0045 - male_female_loss: 0.0000e+00Warning: Couldn't load image C://Users//Asus//AnacondaProjects//AFLW//aflw//data//flickr\\2/image09437.jpg\n",
      "100/100 [==============================] - 1205s 12s/step - loss: 0.0044 - face_nonface_loss: 8.7822e-06 - landmarks_loss: 1.8487e-04 - visibility_loss: 3.4269e-04 - roll_pitch_yaw_loss: 0.0039 - male_female_loss: 0.0000e+00 - val_loss: 0.0015 - val_face_nonface_loss: 7.3849e-06 - val_landmarks_loss: 7.5267e-06 - val_visibility_loss: 1.7435e-04 - val_roll_pitch_yaw_loss: 0.0013 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 1214s 12s/step - loss: 0.0018 - face_nonface_loss: 5.4893e-06 - landmarks_loss: 7.2457e-06 - visibility_loss: 1.3404e-04 - roll_pitch_yaw_loss: 0.0017 - male_female_loss: 0.0000e+00 - val_loss: 0.0010 - val_face_nonface_loss: 3.1860e-06 - val_landmarks_loss: 3.0431e-06 - val_visibility_loss: 6.9878e-05 - val_roll_pitch_yaw_loss: 9.4412e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 1183s 12s/step - loss: 0.0012 - face_nonface_loss: 3.0374e-06 - landmarks_loss: 3.2521e-06 - visibility_loss: 7.1035e-05 - roll_pitch_yaw_loss: 0.0011 - male_female_loss: 0.0000e+00 - val_loss: 7.4735e-04 - val_face_nonface_loss: 1.7367e-06 - val_landmarks_loss: 1.9755e-06 - val_visibility_loss: 3.8416e-05 - val_roll_pitch_yaw_loss: 7.0522e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 1124s 11s/step - loss: 8.6191e-04 - face_nonface_loss: 1.9781e-06 - landmarks_loss: 1.7065e-06 - visibility_loss: 4.4756e-05 - roll_pitch_yaw_loss: 8.1347e-04 - male_female_loss: 0.0000e+00 - val_loss: 5.8257e-04 - val_face_nonface_loss: 1.0541e-06 - val_landmarks_loss: 1.5277e-06 - val_visibility_loss: 2.4613e-05 - val_roll_pitch_yaw_loss: 5.5538e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 1111s 11s/step - loss: 6.4547e-04 - face_nonface_loss: 1.2959e-06 - landmarks_loss: 1.3149e-06 - visibility_loss: 2.9485e-05 - roll_pitch_yaw_loss: 6.1338e-04 - male_female_loss: 0.0000e+00 - val_loss: 4.7092e-04 - val_face_nonface_loss: 6.6872e-07 - val_landmarks_loss: 1.0248e-06 - val_visibility_loss: 1.5761e-05 - val_roll_pitch_yaw_loss: 4.5347e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 1110s 11s/step - loss: 5.1309e-04 - face_nonface_loss: 9.4224e-07 - landmarks_loss: 8.7250e-07 - visibility_loss: 2.0760e-05 - roll_pitch_yaw_loss: 4.9052e-04 - male_female_loss: 0.0000e+00 - val_loss: 4.2278e-04 - val_face_nonface_loss: 5.4365e-07 - val_landmarks_loss: 7.8223e-07 - val_visibility_loss: 1.2577e-05 - val_roll_pitch_yaw_loss: 4.0887e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 1106s 11s/step - loss: 4.1619e-04 - face_nonface_loss: 7.4907e-07 - landmarks_loss: 6.6013e-07 - visibility_loss: 1.5863e-05 - roll_pitch_yaw_loss: 3.9892e-04 - male_female_loss: 0.0000e+00 - val_loss: 2.9647e-04 - val_face_nonface_loss: 4.3379e-07 - val_landmarks_loss: 5.8867e-07 - val_visibility_loss: 9.7106e-06 - val_roll_pitch_yaw_loss: 2.8574e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 1109s 11s/step - loss: 3.3647e-04 - face_nonface_loss: 6.1846e-07 - landmarks_loss: 4.3540e-07 - visibility_loss: 1.2308e-05 - roll_pitch_yaw_loss: 3.2311e-04 - male_female_loss: 0.0000e+00 - val_loss: 2.5607e-04 - val_face_nonface_loss: 3.7431e-07 - val_landmarks_loss: 4.4281e-07 - val_visibility_loss: 8.0348e-06 - val_roll_pitch_yaw_loss: 2.4721e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 1105s 11s/step - loss: 2.7663e-04 - face_nonface_loss: 5.1938e-07 - landmarks_loss: 3.0353e-07 - visibility_loss: 9.8866e-06 - roll_pitch_yaw_loss: 2.6592e-04 - male_female_loss: 0.0000e+00 - val_loss: 2.2250e-04 - val_face_nonface_loss: 3.1431e-07 - val_landmarks_loss: 3.3357e-07 - val_visibility_loss: 6.3436e-06 - val_roll_pitch_yaw_loss: 2.1551e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 1111s 11s/step - loss: 2.3198e-04 - face_nonface_loss: 4.3558e-07 - landmarks_loss: 2.2577e-07 - visibility_loss: 7.8431e-06 - roll_pitch_yaw_loss: 2.2347e-04 - male_female_loss: 0.0000e+00 - val_loss: 1.7814e-04 - val_face_nonface_loss: 2.7396e-07 - val_landmarks_loss: 2.5318e-07 - val_visibility_loss: 5.2673e-06 - val_roll_pitch_yaw_loss: 1.7234e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 1106s 11s/step - loss: 1.9590e-04 - face_nonface_loss: 3.7321e-07 - landmarks_loss: 1.7422e-07 - visibility_loss: 6.4098e-06 - roll_pitch_yaw_loss: 1.8895e-04 - male_female_loss: 0.0000e+00 - val_loss: 1.5408e-04 - val_face_nonface_loss: 2.4527e-07 - val_landmarks_loss: 1.9701e-07 - val_visibility_loss: 4.4151e-06 - val_roll_pitch_yaw_loss: 1.4923e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 1105s 11s/step - loss: 1.6566e-04 - face_nonface_loss: 3.2718e-07 - landmarks_loss: 1.3496e-07 - visibility_loss: 5.3142e-06 - roll_pitch_yaw_loss: 1.5988e-04 - male_female_loss: 0.0000e+00 - val_loss: 1.4139e-04 - val_face_nonface_loss: 2.1802e-07 - val_landmarks_loss: 1.6009e-07 - val_visibility_loss: 3.7335e-06 - val_roll_pitch_yaw_loss: 1.3728e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 1092s 11s/step - loss: 1.4137e-04 - face_nonface_loss: 2.8258e-07 - landmarks_loss: 9.0280e-08 - visibility_loss: 4.3267e-06 - roll_pitch_yaw_loss: 1.3667e-04 - male_female_loss: 0.0000e+00 - val_loss: 1.1488e-04 - val_face_nonface_loss: 2.0026e-07 - val_landmarks_loss: 1.2659e-07 - val_visibility_loss: 3.2560e-06 - val_roll_pitch_yaw_loss: 1.1129e-04 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 1107s 11s/step - loss: 1.2139e-04 - face_nonface_loss: 2.5596e-07 - landmarks_loss: 8.5430e-08 - visibility_loss: 3.7519e-06 - roll_pitch_yaw_loss: 1.1729e-04 - male_female_loss: 0.0000e+00 - val_loss: 1.0005e-04 - val_face_nonface_loss: 1.8671e-07 - val_landmarks_loss: 1.0274e-07 - val_visibility_loss: 2.8731e-06 - val_roll_pitch_yaw_loss: 9.6888e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 1105s 11s/step - loss: 1.0418e-04 - face_nonface_loss: 2.2894e-07 - landmarks_loss: 6.3868e-08 - visibility_loss: 3.1392e-06 - roll_pitch_yaw_loss: 1.0075e-04 - male_female_loss: 0.0000e+00 - val_loss: 8.8715e-05 - val_face_nonface_loss: 1.7249e-07 - val_landmarks_loss: 8.3305e-08 - val_visibility_loss: 2.4934e-06 - val_roll_pitch_yaw_loss: 8.5966e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 1092s 11s/step - loss: 9.1799e-05 - face_nonface_loss: 2.0865e-07 - landmarks_loss: 4.6448e-08 - visibility_loss: 2.6980e-06 - roll_pitch_yaw_loss: 8.8846e-05 - male_female_loss: 0.0000e+00 - val_loss: 7.3862e-05 - val_face_nonface_loss: 1.6225e-07 - val_landmarks_loss: 6.2347e-08 - val_visibility_loss: 2.2032e-06 - val_roll_pitch_yaw_loss: 7.1434e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1086s 11s/step - loss: 7.8375e-05 - face_nonface_loss: 1.8939e-07 - landmarks_loss: 3.5160e-08 - visibility_loss: 2.2729e-06 - roll_pitch_yaw_loss: 7.5877e-05 - male_female_loss: 0.0000e+00 - val_loss: 6.5074e-05 - val_face_nonface_loss: 1.5162e-07 - val_landmarks_loss: 5.1947e-08 - val_visibility_loss: 1.9519e-06 - val_roll_pitch_yaw_loss: 6.2918e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 1111s 11s/step - loss: 7.1333e-05 - face_nonface_loss: 1.7759e-07 - landmarks_loss: 2.2936e-08 - visibility_loss: 2.0085e-06 - roll_pitch_yaw_loss: 6.9124e-05 - male_female_loss: 0.0000e+00 - val_loss: 6.2196e-05 - val_face_nonface_loss: 1.4944e-07 - val_landmarks_loss: 4.3908e-08 - val_visibility_loss: 1.8507e-06 - val_roll_pitch_yaw_loss: 6.0152e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 1091s 11s/step - loss: 6.0149e-05 - face_nonface_loss: 1.6660e-07 - landmarks_loss: 2.1534e-08 - visibility_loss: 1.7741e-06 - roll_pitch_yaw_loss: 5.8187e-05 - male_female_loss: 0.0000e+00 - val_loss: 5.9853e-05 - val_face_nonface_loss: 1.4211e-07 - val_landmarks_loss: 3.3035e-08 - val_visibility_loss: 1.6581e-06 - val_roll_pitch_yaw_loss: 5.8019e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 1094s 11s/step - loss: 5.6378e-05 - face_nonface_loss: 1.5719e-07 - landmarks_loss: 1.8380e-08 - visibility_loss: 1.5875e-06 - roll_pitch_yaw_loss: 5.4615e-05 - male_female_loss: 0.0000e+00 - val_loss: 4.4076e-05 - val_face_nonface_loss: 1.4129e-07 - val_landmarks_loss: 2.8443e-08 - val_visibility_loss: 1.6361e-06 - val_roll_pitch_yaw_loss: 4.2271e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 1090s 11s/step - loss: 4.5724e-05 - face_nonface_loss: 1.5396e-07 - landmarks_loss: 1.2732e-08 - visibility_loss: 1.4226e-06 - roll_pitch_yaw_loss: 4.4135e-05 - male_female_loss: 0.0000e+00 - val_loss: 4.1887e-05 - val_face_nonface_loss: 1.3906e-07 - val_landmarks_loss: 2.0981e-08 - val_visibility_loss: 1.4308e-06 - val_roll_pitch_yaw_loss: 4.0296e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 1090s 11s/step - loss: 4.2320e-05 - face_nonface_loss: 1.4338e-07 - landmarks_loss: 7.2118e-09 - visibility_loss: 1.2805e-06 - roll_pitch_yaw_loss: 4.0889e-05 - male_female_loss: 0.0000e+00 - val_loss: 3.4871e-05 - val_face_nonface_loss: 1.3220e-07 - val_landmarks_loss: 1.8999e-08 - val_visibility_loss: 1.3312e-06 - val_roll_pitch_yaw_loss: 3.3389e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 1102s 11s/step - loss: 3.4833e-05 - face_nonface_loss: 1.3978e-07 - landmarks_loss: 8.3070e-09 - visibility_loss: 1.0749e-06 - roll_pitch_yaw_loss: 3.3610e-05 - male_female_loss: 0.0000e+00 - val_loss: 3.2053e-05 - val_face_nonface_loss: 1.3010e-07 - val_landmarks_loss: 1.5955e-08 - val_visibility_loss: 1.1536e-06 - val_roll_pitch_yaw_loss: 3.0754e-05 - val_male_female_loss: 0.0000e+00\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 1089s 11s/step - loss: 3.2511e-05 - face_nonface_loss: 1.3973e-07 - landmarks_loss: 4.3075e-09 - visibility_loss: 1.0083e-06 - roll_pitch_yaw_loss: 3.1359e-05 - male_female_loss: 0.0000e+00 - val_loss: 2.5796e-05 - val_face_nonface_loss: 1.2987e-07 - val_landmarks_loss: 1.0616e-08 - val_visibility_loss: 1.0722e-06 - val_roll_pitch_yaw_loss: 2.4583e-05 - val_male_female_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import hf\n",
    "\n",
    "print(\"Building train/validation data generator...\")\n",
    "\n",
    "# para corregir el problema de una imagen jpg no válida:\n",
    "# convert -resize 50% /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg\n",
    "# convert -resize 200% /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg /home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image21068.jpg\n",
    "json_dir = os.path.dirname(os.path.realpath('__file__')) # genérico\n",
    "\n",
    "train_data = hf.ImageDataGeneratorV2(samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'negatives5k-train.json', 'positives5k-train.json', \n",
    "                                                 # 'positives.json', 'negatives.json', \n",
    "                                                 pos_max_load_labels=12116, neg_max_load_labels=12116*3, #Número de negativos 454512\n",
    "                                                 output_type='hyperface', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "val_data = hf.ImageDataGeneratorV2(samplewise_center=True,            \n",
    "                                    samplewise_std_normalization=True)\n",
    "val_data_flow = val_data.flow_from_directory(json_dir, \n",
    "                                              'negatives5k-test.json', 'positives5k-test.json', \n",
    "                                              pos_max_load_labels=505, neg_max_load_labels=505*3, #Número de negativos 18938\n",
    "                                              output_type='hyperface', target_size=(227, 227),\n",
    "                                              pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "# # checkpoint\n",
    "# filepath=\"weights-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='train_loss', verbose=1, save_best_only=False, mode='min', period=30)\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "print(\"Start training...\")\n",
    "# history = hyperface.fit_generator(train_data_flow, steps_per_epoch=100, epochs=300, callbacks=callbacks_list)\n",
    "history = hyperface.fit_generator(generator=train_data_flow, \n",
    "                                   validation_data=val_data_flow,\n",
    "                                   validation_steps=100,\n",
    "                                  steps_per_epoch=100, epochs=25) \n",
    "#                                   steps_per_epoch=100, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12116 positive samples and 12116 negative samples.\n",
      "Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)\n",
      "face_nonface ground truth:\n",
      "[0 1]\n",
      "face_nonface predicted:\n",
      "[[  4.53685232e-08   1.00000000e+00]]\n",
      "\n",
      "landmarks ground truth:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "landmarks predicted:\n",
      "[[ -1.16454665e-07  -3.13893977e-08   7.63012849e-06   3.28865866e-07\n",
      "   -1.11974828e-06  -6.68034409e-06  -4.32044629e-07   7.62155469e-06\n",
      "    1.80779568e-06   2.26687462e-07   1.14269489e-04  -3.95842144e-05\n",
      "   -5.30707766e-05  -1.46989887e-05   9.11433631e-07   1.14984584e-06\n",
      "   -1.40033819e-06  -9.86578016e-07  -5.76703371e-07   4.01092757e-06\n",
      "   -7.51229963e-05  -4.60335104e-05  -3.79526028e-07  -2.97709448e-05\n",
      "   -4.13479465e-05   8.86376620e-06   5.06723723e-07   3.45885127e-07\n",
      "   -9.39209031e-08  -6.47640150e-07   2.68219537e-07   5.91572530e-07\n",
      "   -1.23248674e-06   3.22521714e-06  -3.09705160e-06  -1.18114963e-06\n",
      "    5.34406863e-05  -6.18425383e-06   1.23528705e-04  -2.14441866e-06\n",
      "   -1.65942747e-05  -3.00756369e-07]]\n",
      "\n",
      "visibility ground truth:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "visibility predicted:\n",
      "[[ 0.00109484  0.00151164  0.0011162   0.00106342  0.00100714  0.00064393\n",
      "   0.00095864  0.00063167  0.00098998  0.0011409   0.00108598  0.00113402\n",
      "   0.00093383  0.00101231  0.00109472  0.00051826  0.00096094  0.00134004\n",
      "   0.00045297  0.00048707  0.00090509]]\n",
      "\n",
      "pose ground truth:\n",
      "[ 0.  0.  0.]\n",
      "pose predicted:\n",
      "[[-0.00557567 -0.00012834  0.00289657]]\n",
      "\n",
      "gender ground truth:\n",
      "[0 0]\n",
      "gender predicted:\n",
      "[[ 0.85385203  0.146148  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                            'negatives5k-train.json', 'positives5k-train.json', \n",
    "                                            # 'positives.json', 'negatives.json', \n",
    "                                            pos_max_load_labels=12116, neg_max_load_labels=12116*3, #Número de negativos 454512\n",
    "                                            output_type='predict', target_size=(227, 227),\n",
    "                                            pos_batch_size=64, neg_batch_size=64)\n",
    "batch_x, batch_image, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = data_flow.next()\n",
    "\n",
    "# DO NOT LOAD LIKE THIS, USE A REGION FIRST!\n",
    "# path = batch_image[0]\n",
    "# print(\"loading\", path)\n",
    "# img = image.load_img(path, target_size=(227, 227))\n",
    "# x = image.img_to_array(img)\n",
    "# x = train_data.standardize(x)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "\n",
    "subject = 0\n",
    "x = batch_x[subject]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = hyperface.predict(x)\n",
    "\n",
    "print('Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)')\n",
    "\n",
    "print('face_nonface ground truth:')\n",
    "print(batch_y_fnf[subject])\n",
    "print('face_nonface predicted:')\n",
    "print(preds[0])\n",
    "print()\n",
    "\n",
    "print('landmarks ground truth:')\n",
    "print(batch_y_landmarks[subject])\n",
    "print('landmarks predicted:')\n",
    "print(preds[1])\n",
    "print()\n",
    "\n",
    "print('visibility ground truth:')\n",
    "print(batch_y_visfac[subject])\n",
    "print('visibility predicted:')\n",
    "print(preds[2])\n",
    "print()\n",
    "\n",
    "print('pose ground truth:')\n",
    "print(batch_y_pose[subject])\n",
    "print('pose predicted:')\n",
    "print(preds[3])\n",
    "print()\n",
    "\n",
    "print('gender ground truth:')\n",
    "print(batch_y_gender[subject])\n",
    "print('gender predicted:')\n",
    "print(preds[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda2\\envs\\keras\\lib\\site-packages\\pandas\\plotting\\_core.py:1714: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  series.name = label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x16383ff52b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAEICAYAAABLQKIlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7xl6PGLlMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x163844222b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(history.history)\n",
    "# df.plot(y=['loss', 'val_loss'], figsize=(16,4), title='Loss')\n",
    "# df.plot(y=['acc', 'val_acc'], figsize=(16,4), title='Accuracy')\n",
    "df.plot(y=['loss'], figsize=(12,4), title='Loss')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "hyperface.save_weights(\"hyperface.weights.h5\")\n",
    "\n",
    "model_yaml = hyperface.to_yaml()\n",
    "with open(\"hyperface.model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "# throws json non serializable exception\n",
    "# model_json = hyperface.to_json()\n",
    "# with open(\"hyperface.model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "print(\"model written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a model without heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperface_faceposegender model compiled\n",
      "Found 2 positive samples and 2 negative samples.\n",
      "training...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1951e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 3s - loss: 0.0188 - face_nonface_loss: 1.1921e-07 - roll_pitch_yaw_loss: 0.0188 - male_female_loss: 2.9802e-08     \n"
     ]
    }
   ],
   "source": [
    "hyperface_faceposegender = Model(inputs=model.input, outputs=[face_nonface, roll_pitch_yaw, male_female])\n",
    "optimizer = Adam(lr=0.0001)\n",
    "hyperface_faceposegender.compile(optimizer=optimizer,\n",
    "                  loss={\n",
    "                      'face_nonface': 'categorical_crossentropy',\n",
    "                      'roll_pitch_yaw': custom_loss_pose,\n",
    "                      'male_female': 'categorical_crossentropy'},\n",
    "                  loss_weights={\n",
    "                      'face_nonface': 1,\n",
    "                      'roll_pitch_yaw': 1,\n",
    "                      'male_female': 1})\n",
    "print(\"hyperface_faceposegender model compiled\")\n",
    "\n",
    "json_dir = os.path.dirname(os.path.realpath('__file__')) # genérico\n",
    "train_data = hf.ImageDataGeneratorV2(samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json3k', 'negatives.json3k', \n",
    "                                                 # 'positives.json', 'negatives.json', \n",
    "                                                 pos_max_load_labels=2, neg_max_load_labels=2,\n",
    "                                                 output_type='faceposegender', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "print(\"training...\")\n",
    "history = hyperface_faceposegender.fit_generator(generator=train_data_flow, \n",
    "                                  steps_per_epoch=100, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 positive samples and 1 negative samples.\n",
      "Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)\n",
      "face_nonface ground truth:\n",
      "[1 0]\n",
      "face_nonface predicted:\n",
      "[[  9.86067116e-01   6.11221317e-12]]\n",
      "\n",
      "pose ground truth:\n",
      "[-0.12002859 -0.22056605 -0.40335208]\n",
      "pose predicted:\n",
      "[[  2.77214002e-10   3.17766813e-10   7.23532623e-11]]\n",
      "\n",
      "gender ground truth:\n",
      "[0 1]\n",
      "gender predicted:\n",
      "[[  6.42394287e-13   1.00000000e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                           'positives.json3k', 'negatives.json3k', \n",
    "                                           # 'positives.json', 'negatives.json', \n",
    "                                           pos_max_load_labels=1, neg_max_load_labels=1,\n",
    "                                           output_type='predict', target_size=(227, 227),\n",
    "                                           pos_batch_size=64, neg_batch_size=64)\n",
    "batch_x, batch_image, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = data_flow.next()\n",
    "\n",
    "# DO NOT LOAD LIKE THIS, USE A REGION FIRST!\n",
    "# path = batch_image[0]\n",
    "# print(\"loading\", path)\n",
    "# img = image.load_img(path, target_size=(227, 227))\n",
    "# x = image.img_to_array(img)\n",
    "# x = train_data.standardize(x)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "\n",
    "x = batch_x[0]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = hyperface_faceposegender.predict(x)\n",
    "\n",
    "print('Predicciones: (face_nonface, landmarks, visibility, roll_pitch_yaw, male_female)')\n",
    "\n",
    "print('face_nonface ground truth:')\n",
    "print(batch_y_fnf[0])\n",
    "print('face_nonface predicted:')\n",
    "print(preds[0])\n",
    "print()\n",
    "\n",
    "print('pose ground truth:')\n",
    "print(batch_y_pose[0])\n",
    "print('pose predicted:')\n",
    "print(preds[1])\n",
    "print()\n",
    "\n",
    "print('gender ground truth:')\n",
    "print(batch_y_gender[0])\n",
    "print('gender predicted:')\n",
    "print(preds[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02123045', 'tabby', 0.8085466), ('n02123159', 'tiger_cat', 0.14428307), ('n02124075', 'Egyptian_cat', 0.043808475), ('n02971356', 'carton', 0.0009396731), ('n02127052', 'lynx', 0.00060345326)]]\n"
     ]
    }
   ],
   "source": [
    "model = squeeze.SqueezeNet()\n",
    "\n",
    "path = '/home/lmiguel/Projects/datasets/coco/images/test2017/000000000665.jpg'\n",
    "img = image.load_img(path, target_size=(227, 227))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
