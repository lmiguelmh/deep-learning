{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import squeeze\n",
    "import tensorflow as tf\n",
    "import keras.backend as kb\n",
    "import keras.losses as losses\n",
    "import hf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 positive samples and 2 negative samples.\n",
      "Found 2 positive samples and 2 negative samples.\n"
     ]
    }
   ],
   "source": [
    "json_dir = '/home/lmiguel/Projects/deep-learning/hyperface'\n",
    "\n",
    "# data de entrenamiento normalizada\n",
    "norm_train_data = hf.ImageDataGeneratorV2(featurewise_center=False,  # requires fit first\n",
    "                                     featurewise_std_normalization=False,  # requires fit first\n",
    "                                     samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "norm_train_data_flow = norm_train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 output_type='hyperface', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "# data de entrenamiento\n",
    "train_data = hf.ImageDataGeneratorV2()\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 # output_type='hyperface',\n",
    "                                                 output_type='predict',\n",
    "                                                 target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pérdida para visibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809523809524\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def custom_loss_visibility(y_true, y_pred):\n",
    "    # en paper: loss = 1/N * sum((vpredi-vi)^2)\n",
    "    # para entrenar:\n",
    "    # return (1/21) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    return (1/21) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = norm_train_data_flow.next() \n",
    "vis = batch_y_visfac[0]\n",
    "vis2 = np.zeros(vis.shape)\n",
    "print(custom_loss_visibility(vis,vis2))\n",
    "print(custom_loss_visibility(vis,vis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pérdida para pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191899001485\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def custom_loss_pose(y_true, y_pred):\n",
    "    # en paper: loss = 1/3 * sum((ppredi-pi)^2)\n",
    "    # para entrenamiento:\n",
    "    # return (1/3) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    return (1/3) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = norm_train_data_flow.next() \n",
    "pose = batch_y_pose[0]\n",
    "pose2 = np.zeros(pose.shape)\n",
    "print(custom_loss_pose(pose,pose2))\n",
    "print(custom_loss_pose(pose,pose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pérdida para landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030594314446\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def custom_loss_landmarks(coord_true, coord_pred):    \n",
    "    # en paper: loss = 1/(2N) * Sum(vi*((xpredi-ai)^2 + (ypredi-bi)^2))\n",
    "    x_true_coord = coord_true[0:21]\n",
    "    y_true_coord = coord_true[21:42]\n",
    "    viz_true = coord_true[42:63]\n",
    "    x_pred_coord = coord_pred[0:21]\n",
    "    y_pred_coord = coord_pred[21:42]\n",
    "    # para entrenamiento:\n",
    "    # return (1/(2*21)) * kb.sum(viz_true * (kb.square(x_pred_coord-x_true_coord) + K.square(y_pred_coord - y_true_coord)), axis=-1)\n",
    "    # para debug:\n",
    "    return (1/(2*21)) * np.sum(viz_true * ((x_pred_coord-x_true_coord)**2 + (y_pred_coord - y_true_coord)**2), axis=-1)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = norm_train_data_flow.next()\n",
    "landmarks = batch_y_landmarks[0]\n",
    "landmarks2 = np.zeros(landmarks.shape)\n",
    "print(custom_loss_landmarks(landmarks, landmarks2))\n",
    "print(custom_loss_landmarks(landmarks, landmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = '/home/lmiguel/Projects/deep-learning/hyperface'\n",
    "train_data = hf.ImageDataGeneratorV2(featurewise_center=False,  # requires fit first\n",
    "                                     featurewise_std_normalization=False,  # requires fit first\n",
    "                                     samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 output_type='hyperface', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = train_data_flow.next()\n",
    "# print( batch_y_landmarks)\n",
    "landmarks_true = batch_y_landmarks[0]\n",
    "# print(landmarks_true)\n",
    "print(batch_x[0])\n",
    "landmarks_pred = np.zeros(landmarks_true.shape)\n",
    "cost = custom_loss_landmarks(landmarks_true, landmarks_pred)\n",
    "print(cost)\n",
    "cost = custom_loss_landmarks(landmarks_true, landmarks_true)\n",
    "print(cost)\n",
    "\n",
    "\n",
    "# img = batch_x[0]\n",
    "# landmarks = np.dstack([landmarks_true[0:21],landmarks_true[21:42]])[0]\n",
    "# for pt in landmarks:\n",
    "#     pt = (int(pt[0]), int(pt[1]))\n",
    "#     cv2.circle(img, pt, 4, (255,255,255), -1)\n",
    "# plt.figure()\n",
    "# plt.imshow(img)\n",
    "\n",
    "\n",
    "train_data = hf.ImageDataGeneratorV2()\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 # output_type='hyperface',\n",
    "                                                 output_type='predict',\n",
    "                                                 target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "# batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = train_data_flow.next()\n",
    "batch_x, _, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = train_data_flow.next()\n",
    "landmarks_true = batch_y_landmarks[0]\n",
    "# landmarks_pred = np.zeros(landmarks_true.shape)\n",
    "# print(landmarks_true)\n",
    "# print(batch_bbox[0])\n",
    "\n",
    "# path = '/home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image00035.jpg'\n",
    "# img = cv2.imread(path)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# landmarks = np.dstack([landmarks_true[0:21],landmarks_true[21:42]])[0]\n",
    "# for pt in landmarks:\n",
    "#     pt = (int(pt[0]), int(pt[1]))\n",
    "#     cv2.circle(img, pt, 4, (255,255,255), -1)\n",
    "# plt.figure()\n",
    "# plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list([1])\n",
    "n = np.array([2,3])\n",
    "a.extend(n)\n",
    "# print(a)\n",
    "np.array(a)\n",
    "np.append(n,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other custom loss wich are wrong!\n",
    "# loss from hyperface-with-squeezenet\n",
    "def custom_mse_lm(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))*kb.sum(kb.square(tf.multiply((kb.sign(y_true)+1)*0.5, y_true-y_pred)),axis=-1)/kb.sum((kb.sign(y_true)+1)*0.5,axis=-1)\n",
    "\n",
    "def custom_mse_pose(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))*losses.mean_squared_error(y_true,y_pred)\n",
    "\n",
    "def custom_mse_lm(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))* kb.sum(kb.square(tf.multiply((kb.sign(y_true)+1)*0.5, y_true-y_pred)),axis=-1)/kb.sum((kb.sign(y_true)+1)*0.5,axis=-1)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return kb.mean(kb.square(y_pred - y_true), axis=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
