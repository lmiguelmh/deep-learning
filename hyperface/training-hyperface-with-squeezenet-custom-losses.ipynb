{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import squeeze\n",
    "import tensorflow as tf\n",
    "import keras.backend as kb\n",
    "import keras.losses as losses\n",
    "import hf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2818 positive samples and 113615 negative samples.\n",
      "Found 2818 positive samples and 113615 negative samples.\n"
     ]
    }
   ],
   "source": [
    "json_dir = os.path.dirname(os.path.realpath('__file__')) # generico\n",
    "\n",
    "# data de entrenamiento normalizada\n",
    "norm_train_data = hf.ImageDataGeneratorV2(featurewise_center=False,  # requires fit first\n",
    "                                     featurewise_std_normalization=False,  # requires fit first\n",
    "                                     samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "norm_train_data_flow = norm_train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 output_type='hyperface', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "# data de entrenamiento\n",
    "train_data = hf.ImageDataGeneratorV2()\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 # output_type='hyperface',\n",
    "                                                 output_type='predict',\n",
    "                                                 target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PÃ©rdida para visibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/home/tf/hyperface/aflw/data/flickr/3/image38301.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c437b65b91cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_y_fnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y_landmarks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y_visfac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y_pose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y_gender\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm_train_data_flow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_y_visfac\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvis2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AnacondaProjects\\deep-learning\\deep-learning\\hyperface\\hf.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcurrent_pos_batch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m                 \u001b[0mbatch_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0ml_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labelFnf\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AnacondaProjects\\deep-learning\\deep-learning\\hyperface\\hf.py\u001b[0m in \u001b[0;36mget_img\u001b[1;34m(self, index, labels, val)\u001b[0m\n\u001b[0;32m    503\u001b[0m                        \u001b[0mgrayscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                        \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m                        bbox=bbox)\n\u001b[0m\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0mimg_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AnacondaProjects\\deep-learning\\deep-learning\\hyperface\\hf.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, target_size, bbox)\u001b[0m\n\u001b[0;32m    298\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m    299\u001b[0m     \u001b[1;31m# print(path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;31m# print(bbox)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\keras\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2530\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/home/tf/hyperface/aflw/data/flickr/3/image38301.jpg'"
     ]
    }
   ],
   "source": [
    "def custom_loss_visibility(y_true, y_pred):\n",
    "    # en paper: loss = 1/N * sum((vpredi-vi)^2)\n",
    "    # para entrenar:\n",
    "    # return (1/21) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    return (1/21) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = norm_train_data_flow.next() \n",
    "vis = batch_y_visfac[0]\n",
    "vis2 = np.zeros(vis.shape)\n",
    "print(custom_loss_visibility(vis,vis2))\n",
    "print(custom_loss_visibility(vis,vis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PÃ©rdida para pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191899001485\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def custom_loss_pose(y_true, y_pred):\n",
    "    # en paper: loss = 1/3 * sum((ppredi-pi)^2)\n",
    "    # para entrenamiento:\n",
    "    # return (1/3) * kb.sum(kb.square(y_pred-y_true), axis=-1)\n",
    "    # para debug:\n",
    "    return (1/3) * np.sum((y_pred-y_true)**2, axis=-1)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = norm_train_data_flow.next() \n",
    "pose = batch_y_pose[0]\n",
    "pose2 = np.zeros(pose.shape)\n",
    "print(custom_loss_pose(pose,pose2))\n",
    "print(custom_loss_pose(pose,pose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PÃ©rdida para landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030594314446\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def custom_loss_landmarks(coord_true, coord_pred):    \n",
    "    # en paper: loss = 1/(2N) * Sum(vi*((xpredi-ai)^2 + (ypredi-bi)^2))\n",
    "    x_true_coord = coord_true[0:21]\n",
    "    y_true_coord = coord_true[21:42]\n",
    "    viz_true = coord_true[42:63]\n",
    "    x_pred_coord = coord_pred[0:21]\n",
    "    y_pred_coord = coord_pred[21:42]\n",
    "    # para entrenamiento:\n",
    "    # return (1/(2*21)) * kb.sum(viz_true * (kb.square(x_pred_coord-x_true_coord) + K.square(y_pred_coord - y_true_coord)), axis=-1)\n",
    "    # para debug:\n",
    "    return (1/(2*21)) * np.sum(viz_true * ((x_pred_coord-x_true_coord)**2 + (y_pred_coord - y_true_coord)**2), axis=-1)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = norm_train_data_flow.next()\n",
    "landmarks = batch_y_landmarks[0]\n",
    "landmarks2 = np.zeros(landmarks.shape)\n",
    "print(custom_loss_landmarks(landmarks, landmarks2))\n",
    "print(custom_loss_landmarks(landmarks, landmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = '/home/lmiguel/Projects/deep-learning/hyperface'\n",
    "train_data = hf.ImageDataGeneratorV2(featurewise_center=False,  # requires fit first\n",
    "                                     featurewise_std_normalization=False,  # requires fit first\n",
    "                                     samplewise_center=True,                                     \n",
    "                                     samplewise_std_normalization=True)\n",
    "\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 output_type='hyperface', target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "\n",
    "batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = train_data_flow.next()\n",
    "# print( batch_y_landmarks)\n",
    "landmarks_true = batch_y_landmarks[0]\n",
    "# print(landmarks_true)\n",
    "print(batch_x[0])\n",
    "landmarks_pred = np.zeros(landmarks_true.shape)\n",
    "cost = custom_loss_landmarks(landmarks_true, landmarks_pred)\n",
    "print(cost)\n",
    "cost = custom_loss_landmarks(landmarks_true, landmarks_true)\n",
    "print(cost)\n",
    "\n",
    "\n",
    "# img = batch_x[0]\n",
    "# landmarks = np.dstack([landmarks_true[0:21],landmarks_true[21:42]])[0]\n",
    "# for pt in landmarks:\n",
    "#     pt = (int(pt[0]), int(pt[1]))\n",
    "#     cv2.circle(img, pt, 4, (255,255,255), -1)\n",
    "# plt.figure()\n",
    "# plt.imshow(img)\n",
    "\n",
    "\n",
    "train_data = hf.ImageDataGeneratorV2()\n",
    "train_data_flow = train_data.flow_from_directory(json_dir,\n",
    "                                                 'positives.json', 'negatives.json', \n",
    "                                                 # output_type='hyperface',\n",
    "                                                 output_type='predict',\n",
    "                                                 target_size=(227, 227),\n",
    "                                                 pos_batch_size=64, neg_batch_size=64)\n",
    "# batch_x, [batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender] = train_data_flow.next()\n",
    "batch_x, _, batch_bbox, batch_y_fnf, batch_y_landmarks, batch_y_visfac, batch_y_pose, batch_y_gender = train_data_flow.next()\n",
    "landmarks_true = batch_y_landmarks[0]\n",
    "# landmarks_pred = np.zeros(landmarks_true.shape)\n",
    "# print(landmarks_true)\n",
    "# print(batch_bbox[0])\n",
    "\n",
    "# path = '/home/lmiguel/Projects/datasets/aflw/aflw/data/flickr/3/image00035.jpg'\n",
    "# img = cv2.imread(path)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# landmarks = np.dstack([landmarks_true[0:21],landmarks_true[21:42]])[0]\n",
    "# for pt in landmarks:\n",
    "#     pt = (int(pt[0]), int(pt[1]))\n",
    "#     cv2.circle(img, pt, 4, (255,255,255), -1)\n",
    "# plt.figure()\n",
    "# plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list([1])\n",
    "n = np.array([2,3])\n",
    "a.extend(n)\n",
    "# print(a)\n",
    "np.array(a)\n",
    "np.append(n,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other custom loss wich are wrong!\n",
    "# loss from hyperface-with-squeezenet\n",
    "def custom_mse_lm(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))*kb.sum(kb.square(tf.multiply((kb.sign(y_true)+1)*0.5, y_true-y_pred)),axis=-1)/kb.sum((kb.sign(y_true)+1)*0.5,axis=-1)\n",
    "\n",
    "def custom_mse_pose(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))*losses.mean_squared_error(y_true,y_pred)\n",
    "\n",
    "def custom_mse_lm(y_true,y_pred):\n",
    "    return kb.sign(kb.sum(kb.abs(y_true),axis=-1))* kb.sum(kb.square(tf.multiply((kb.sign(y_true)+1)*0.5, y_true-y_pred)),axis=-1)/kb.sum((kb.sign(y_true)+1)*0.5,axis=-1)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return kb.mean(kb.square(y_pred - y_true), axis=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
