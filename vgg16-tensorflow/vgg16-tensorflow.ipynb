{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 in Tensorflow\n",
    "#### Very Deep Convolutional Networks for Large Scale Recognition \n",
    "Karen Simonyan & Andrew Zisserman\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "- evaluation of networks of increasing depth with very small (3×3) convolution filters, small enough to capture the notion of left/right, up/down, center\n",
    "- *simple* architecture \n",
    "    - input: 224x224x3\n",
    "    - stacks of convolutional layers with kernel 3x3/stride 1, relu activation, width of 64,128,256,512\n",
    "    - max pooling kernel 2x2/stride 2\n",
    "    - 3 fully connected 4096\n",
    "- 5 configurations were tested (from 8conv/3fc layers to 16 conv/3fc layers)\n",
    "    - VGG-A: 11weight layers -- 133M params -- top1 29.6% / top5 10.4%\n",
    "    - VGG-B: 13wl -- 133M -- top1 28.7% top5 9.9%\n",
    "    - VGG-C: 16wl -- 134M -- top1 28.1% top5 9.4%\n",
    "    - VGG-D: 16wl -- 138M -- top1 27.0% top5 8.8%\n",
    "    - VGG-E: 19wl -- 144M -- top1 27.3% top5 8.0%\n",
    "    - more layers are feasible because of the small convolution filters\n",
    "- training\n",
    "    - mini-batch gradient descent (batch size 256) with momentum (0.9)\n",
    "    - weight decay 0.0005\n",
    "        - regularizer, because of its small ammount it reduces the model’s training error too (AlexNet paper).\n",
    "    - dropout 0.5 for first two fully connected layers\n",
    "    - learning rate 0.01, 0.001, 0.0001, 0.00001\n",
    "    - 74 epochs, less epochs to converge because:\n",
    "        - regularization imposed greater depth and smaller conv filters\n",
    "        - preinitialization - very important\n",
    "            - shallow configurations (configuration A) were initialized randomly\n",
    "            - deeper configuration were initialized with layers of A (intermediate layers were randomly initialized - zero man 0.01 variance, bias 0)\n",
    "            - there are better ways to initialize layers\n",
    "    - augmentation\n",
    "        - random horizontal flipping\n",
    "        - random rgb color shifting\n",
    "        - scale jittering, which improves the performance\n",
    "- implementation\n",
    "    - c++/caffe\n",
    "    - gpu parallelization\n",
    "        - model & data parallelism for different layers\n",
    "    - 2-3 week of training in a 4-gpu system (nvidia titan black gpu)\n",
    "- dataset ILSVRC-2012\n",
    "    - 1000 classes\n",
    "        - 1.3M images training\n",
    "        - 50K validation\n",
    "        - 100K testing\n",
    "- ensemble of D&E configurations\n",
    "    - *best result is achieved by combining just two models*: significantly less than used in most ILSVRC submissions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download the weights and working files\n",
    "- https://www.cs.toronto.edu/~frossard/post/vgg16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "\n",
    "        # lista que almacenará elementos (capas, bias)\n",
    "        # para facilitar la inicialización de pesos luego\n",
    "        self.parameters = None\n",
    "\n",
    "        # construimos las capas según la arquitectura descrita en el paper\n",
    "        # capas convolucionales\n",
    "        self.convlayers()\n",
    "        # capas densas\n",
    "        self.fc_layers()\n",
    "        # activación final\n",
    "        self.probs = tf.nn.softmax(self.fc3l)\n",
    "        \n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    #\n",
    "    # Capas convolucionales\n",
    "    #\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        #\n",
    "        # Construiremos la configuración VGG-D\n",
    "        #\n",
    "\n",
    "        # Preprocesamiento: restamos la media a cada pixel según Pg.2 párrafo 3\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        #\n",
    "        # Bloque 1\n",
    "        # Cada bloque de una CNN está compuesto por 3 elementos:\n",
    "        # 1. Convolución\n",
    "        # 2. Activación\n",
    "        # 3. Pooling\n",
    "        #\n",
    "\n",
    "        # Capa 1: conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            # filtro de 3x3 (Pg. 2 párrafo 3)\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            # [1, 1, 1, 1] - stride 1 (Pg. 2 párrafo 3)\n",
    "            # padding='SAME' - bordes usan el mismo valor de sus vecinos\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            # bias inicializados a 0 (Pg. 4 párrafo 4), no importa serán reescritos luego\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            # k*W + B\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            # activación ReLU para capas internas (Pg. 2, párrafo 5)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            \n",
    "            # añadimos a lista para facilitar inicialización de pesos\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # Capa 2: conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # Pooling: pool1\n",
    "        # max pooling 2x2, stride 2 (Pg. 2, párrafo 3)\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        #\n",
    "        # Bloque 2\n",
    "        #\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        #\n",
    "        # Bloque 3\n",
    "        #\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        #\n",
    "        # Bloque 4\n",
    "        #\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        #\n",
    "        # Bloque 5\n",
    "        #\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "\n",
    "    #\n",
    "    # Capas Densas (fully connected)\n",
    "    #\n",
    "    def fc_layers(self):\n",
    "        # fc1\n",
    "        with tf.name_scope('fc1') as scope:\n",
    "            shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "            # aplanar pesos\n",
    "            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            # aplanar bias\n",
    "            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            #\n",
    "            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "            # convolución como producto de matrices\n",
    "            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "            self.fc1 = tf.nn.relu(fc1l)\n",
    "            self.parameters += [fc1w, fc1b]\n",
    "\n",
    "        # fc2\n",
    "        with tf.name_scope('fc2') as scope:\n",
    "            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "            self.fc2 = tf.nn.relu(fc2l)\n",
    "            self.parameters += [fc2w, fc2b]\n",
    "\n",
    "        # fc3\n",
    "        # la última capa contiene 1000 elementos para 1000 clases\n",
    "        with tf.name_scope('fc3') as scope:\n",
    "            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "            self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        # carga de pesos del modelo usando numpy\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            # print(i, k, np.shape(weights[k]))\n",
    "            # asignar el peso a cada elemento de la lista (capas, bias)\n",
    "            sess.run(self.parameters[i].assign(weights[k]))\n",
    "            # print(self.parameters[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# placeholder para la imagen de 224x244x3\n",
    "imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "\n",
    "# construimos la red y cargamos los pesos\n",
    "vgg = vgg16(imgs, 'vgg16_weights.npz', sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clase:#356 - probabilidad:0.693385\n",
      "clase:#358 - probabilidad:0.175389\n",
      "clase:#357 - probabilidad:0.122085\n",
      "clase:#359 - probabilidad:0.008871\n",
      "clase:#360 - probabilidad:0.000121\n"
     ]
    }
   ],
   "source": [
    "# hacemos una predicción \n",
    "img1 = imread('laska.png', mode='RGB')\n",
    "img1 = imresize(img1, (224, 224))\n",
    "\n",
    "# alimentamos el tensor con la imagen leída\n",
    "# devuelve un vector de predicciones para cada clase\n",
    "prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "\n",
    "# ordenar predicción descendentemente y tomar las 5/1000\n",
    "preds = (np.argsort(prob)[::-1])[0:5]\n",
    "for p in preds:\n",
    "    print(\"clase:#%d - probabilidad:%f\" % (p, prob[p]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
